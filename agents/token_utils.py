# // file: agents/token_utils.py

"""
Utilitaires de gestion du budget de tokens (limite MVP 2000).

R√îLE: √âviter les co√ªts incontr√¥l√©s et timeouts OpenAI en validant 
la taille du prompt AVANT l'appel API.

TRADE-OFFS: Approximation simple vs tokenizer exact (tiktoken), 
mais suffisant pour la limite MVP et compatible avec toutes les versions OpenAI.
"""

# Inventaire des d√©pendances
# - re (stdlib) : expressions r√©guli√®res pour nettoyage texte ‚Äî alternative: str.replace() mais moins flexible
# - typing (stdlib) : annotations de types ‚Äî am√©liore lisibilit√© du code
# - fastapi (tierce) : HTTPException pour erreurs API ‚Äî standard FastAPI pour retour utilisateur
import re
from typing import Dict, Any
from fastapi import HTTPException


# Constantes MVP (selon MVP.md)
MAX_TOKENS_PER_REQUEST = 2000  # Budget strict par requ√™te
TOKEN_SAFETY_MARGIN = 200      # Marge pour la r√©ponse OpenAI
MAX_PROMPT_TOKENS = MAX_TOKENS_PER_REQUEST - TOKEN_SAFETY_MARGIN  # 1800 tokens max pour prompt


def estimate_tokens(text: str) -> int:
    """
    Estimation rapide du nombre de tokens (approximation 1 token ‚âà 4 caract√®res).
    
    Alternative plus pr√©cise: tiktoken d'OpenAI, mais ajoute une d√©pendance lourde.
    L'approximation 4 chars/token est suffisante pour la validation MVP.
    
    Args:
        text: Texte √† analyser
        
    Returns:
        Nombre estim√© de tokens
    """
    if not text or not text.strip():
        return 0
    
    # Nettoyage basique (espaces multiples, retours √† la ligne)
    cleaned = re.sub(r'\s+', ' ', text.strip())
    
    # Estimation conservative: 4 caract√®res ‚âà 1 token
    # Plus pr√©cis que "1 mot = 1 token" car tient compte de la ponctuation
    estimated_tokens = len(cleaned) // 4
    
    # Marge d'erreur +20% pour √™tre conservateur
    return int(estimated_tokens * 1.2)


def validate_prompt_budget(prompt: str, context: Dict[str, Any] = None) -> None:
    """
    Valide que le prompt respecte le budget de tokens MVP.
    
    L√®ve HTTPException si d√©passement pour retourner une erreur claire √† l'utilisateur.
    
    Args:
        prompt: Prompt complet √† envoyer √† OpenAI
        context: Contexte pour logs (subject, audience, etc.)
        
    Raises:
        HTTPException: Si prompt trop long (413 Request Entity Too Large)
    """
    if not prompt:
        return
        
    token_count = estimate_tokens(prompt)
    
    if token_count > MAX_PROMPT_TOKENS:
        # Contexte pour erreur utilisateur
        ctx_info = ""
        if context:
            subject = context.get("subject", "")[:50]
            if subject:
                ctx_info = f" pour '{subject}'"
                
        raise HTTPException(
            status_code=413,  # Request Entity Too Large
            detail=(
                f"‚ö†Ô∏è Prompt trop long: {token_count} tokens (limite: {MAX_PROMPT_TOKENS})\n\n"
                f"üõ†Ô∏è Solutions{ctx_info}:\n"
                f"‚Ä¢ Raccourcir le sujet (actuellement: {len(context.get('subject', ''))} caract√®res)\n"
                f"‚Ä¢ Choisir une dur√©e plus courte\n"
                f"‚Ä¢ Reformuler avec des termes plus simples\n\n"
                f"üí° Budget MVP: {MAX_TOKENS_PER_REQUEST} tokens max par le√ßon"
            )
        )